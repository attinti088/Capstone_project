---
title: 'Coursera Data Science Capstone: Exploratory Data Analysis'
author: "Venkatesh Attinti"
date: "5/26/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Executive Summary
This milestone report is based on the exploratory data analysis of the swift key data provided in context of Data science capstone project.The data consist of 3 data file from different sources - (twitter,blogs,news).This report showcases the tidytext approach used for data analysis.More information regarding the tidy text approach can be accessed from here https://www.tidytextmining.com/

## Data Summary
It is assumed that the data from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip is downloaded, unziped and available in the working directory.

### Below is the summary of the data loaded
```{r,warning=FALSE}
library("stringi")

twitterRawData <- readLines("en_US.twitter.txt",warn=FALSE,encoding="UTF-8")
blogsRawData <- readLines("en_US.blogs.txt",warn=FALSE,encoding="UTF-8")
newsRawData <- readLines("en_US.news.txt",warn=FALSE,encoding="UTF-8")

datasummary <- data.frame("FileNames" =c("Twitter","Blogs","News"),
                          "FileSize"=c(format(object.size(twitterRawData), units = "MB", standard ="auto"),
                                       format(object.size(blogsRawData), units = "MB", standard = "auto"),
                                       format(object.size(newsRawData), units = "MB", standard = "auto")),
                          "FileLength"=c(length(twitterRawData),length(blogsRawData),length(newsRawData)),
                          "Wordcount"=c(sum(stri_stats_latex(twitterRawData)[4]),sum(stri_stats_latex(blogsRawData)[4]),sum(stri_stats_latex(newsRawData)[4])),
                          "NoOfChars"=c(sum(nchar(twitterRawData)),sum(nchar(blogsRawData)),sum(nchar(newsRawData))))

datasummary
```

## Exploratory Data Analysis
In this section we will perform some exploratory data analysis using tidy data principles which is a powerful way to make handling data easier and more effective.

we will perform this analysis on the sample data set which is 2% of the original dataset.

Below are  packages requried to perform this analysis
library("tidyr")
library("dplyr")
library("tidytext")
library("tm")
library("openNLP")
library("RWeka")
library("tm")

```{r,warning=FALSE}
# Remove all non english characters as they cause issues down the road
twitterRawData <- iconv(twitterRawData, "latin1", "ASCII", sub="")
blogsRawData <- iconv(blogsRawData, "latin1", "ASCII", sub="")
newsRawData <- iconv(newsRawData, "latin1", "ASCII", sub="")

#sampling of the data set 
set.seed(2020)
twitterRawData_sample<- sample(twitterRawData,length(twitterRawData)*0.02)
blogsRawData_sample<- sample(blogsRawData,length(blogsRawData)*0.02)
newsRawData_sample<- sample(newsRawData,length(newsRawData)*0.02)

#write the sample files

dir.create("sampleDatafiles", showWarnings = FALSE)

write(twitterRawData_sample, "sampleDatafiles/twitterRawData_sample.txt")
write(blogsRawData_sample, "sampleDatafiles/blogsRawData_sample.txt")
write(newsRawData_sample, "sampleDatafiles/newsRawData_sample.txt")


remove(twitterRawData)
remove(blogsRawData)
remove(newsRawData)

```

Merging the sample data files into single corpus and then converting to tibble data frame format which will be used in all the further analysis
```{r,warning=FALSE}
library("tidyr")
library("dplyr")
library("tidytext")
library("tm")
library("openNLP")
library("RWeka")
library("tm")
finalSampleData <- c(twitterRawData_sample,blogsRawData_sample,newsRawData_sample)
sampleData <- tibble(text = finalSampleData)

```

Pre-processing the data(invloves operations like removing the whitespaces, punctuation,stopwords, stemming etc)
In tidy text the punctuations and converting to lower cases are automatically done during the unnesting tokens.

## Unigrams
Unnesting tokens and removing the stopwords

```{r,warning=FALSE}
data(stop_words)
tidySampleData <- sampleData %>% unnest_tokens(word, text) %>% anti_join(stop_words)
#Removing whitespaces 
tidySampleData$word <- gsub("\\s+","",tidySampleData$word)
#Removing Numbers
tidySampleData<-tidySampleData[-grep("\\b\\d+\\b", tidySampleData$word),]

tidySampleData %>% count(word, sort = TRUE);
```
using tidy tools,the word counts are stored in a tidy data frame.This allows us to pipe directly to the ggplot2 package

```{r,warning=FALSE}
library(ggplot2)
tidySampleData %>% count(word, sort = TRUE) %>% filter(n > 1000) %>% mutate(word = reorder(word, n)) %>% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip();
```
Displaying the most common unnigrams using wordcloud

```{r,warning=FALSE}
library(wordcloud)
library("RColorBrewer")
dark2 <- brewer.pal(5, "Accent")   

tidySampleData %>% count(word) %>% with(wordcloud(word, n, max.words = 100, rot.per=0.1, colors=dark2));
```







### Bigrams -- Tokenizing by 2-gram

```{r,warning=FALSE}
library(tidyr)
stop_words <- rbind(stop_words,data.frame(word="amp",lexicon=""))
tidyBigramSampleData <- sampleData %>% unnest_tokens(bigram, text,token = "ngrams", n = 2) 
#Seperating the bigram 
tidyBigramSampleData_separated <- tidyBigramSampleData %>% separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- tidyBigramSampleData_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)
#Removing whitespaces 
bigrams_filtered$word1 <- gsub("\\s+","",bigrams_filtered$word1)
bigrams_filtered$word2 <- gsub("\\s+","",bigrams_filtered$word2)

bigrams_filtered$word1 <- gsub("\\'+","",bigrams_filtered$word1)
bigrams_filtered$word2 <- gsub("\\'+","",bigrams_filtered$word2)

#Removing Numbers
bigrams_filtered<-bigrams_filtered[-grep("\\b\\d+\\b", bigrams_filtered$word1),]
bigrams_filtered<-bigrams_filtered[-grep("\\b\\d+\\b", bigrams_filtered$word2),]

bigrams_united <- bigrams_filtered %>% unite(bigram, word1, word2, sep = " ")
bigrams_united %>% count(bigram,sort=TRUE)

bigrams_united %>% count(bigram, sort = TRUE) %>% filter(n > 35) %>% mutate(bigram = reorder(bigram, n)) %>% ggplot(aes(bigram, n,fill=n)) + geom_col() + xlab(NULL) + coord_flip()
```



### Visualizing a Network of Bigrams with ggraph
It may be interested in visualizing all of the relationships among words simultaneously,rather than just the top few at a time. As one common visualization, we can arrange the words into a network, or “graph.”
```{r,warning=FALSE}
library(igraph)
bigram_graph <- bigrams_filtered %>% count(word1, word2,sort=TRUE) %>% filter(n > 25) %>% graph_from_data_frame()
library(ggraph)
set.seed(123456)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue",size=3) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

### Trigrams -- Tokenizing by 3-gram

```{r}
library(tidyr)
stop_words <- rbind(stop_words,data.frame(word="amp",lexicon=""))
tidyBigramSampleData <- sampleData %>% unnest_tokens(bigram, text,token = "ngrams", n = 3) 
#Seperating the bigram 
tidyBigramSampleData_separated <- tidyBigramSampleData %>% separate(bigram, c("word1", "word2", "word3"), sep = " ")
bigrams_filtered <- tidyBigramSampleData_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)%>% filter(!word3 %in% stop_words$word)
#Removing whitespaces 
bigrams_filtered$word1 <- gsub("\\s+","",bigrams_filtered$word1)
bigrams_filtered$word2 <- gsub("\\s+","",bigrams_filtered$word2)
bigrams_filtered$word3 <- gsub("\\s+","",bigrams_filtered$word3)

bigrams_filtered$word1 <- gsub("\\'+","",bigrams_filtered$word1)
bigrams_filtered$word2 <- gsub("\\'+","",bigrams_filtered$word2)
bigrams_filtered$word3 <- gsub("\\'+","",bigrams_filtered$word3)

#Removing Numbers
bigrams_filtered<-bigrams_filtered[-grep("\\b\\d+\\b", bigrams_filtered$word1),]
bigrams_filtered<-bigrams_filtered[-grep("\\b\\d+\\b", bigrams_filtered$word2),]
bigrams_filtered<-bigrams_filtered[-grep("\\b\\d+\\b", bigrams_filtered$word3),]

bigrams_united <- bigrams_filtered %>% unite(bigram, word1, word2,word3, sep = " ")
bigrams_united %>% count(bigram,sort=TRUE)

bigrams_united %>% count(bigram, sort = TRUE) %>% filter(n > 10) %>% mutate(bigram = reorder(bigram, n)) %>% ggplot(aes(bigram, n,fill=n)) + geom_col() + xlab(NULL) + coord_flip()
```


### Plan of next steps
I have done the exploratory analysis. The next steps of this capstone project would be to finalize our predictive algorithm, and deploy our algorithm using shiny() app. As for the Shiny app it will consist of a simple user interface that will allow a user to enter text into a single textbox.